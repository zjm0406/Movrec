# recommend_engine/engine.py
import pandas as pd
import numpy as np
import jieba
import re
import os
import pickle # ç”¨äºç¼“å­˜é¢„å¤„ç†æ•°æ®å’Œæ¨¡å‹

# --- TensorFlow/Keras å¯¼å…¥ ---
# ç¡®ä¿ç¯å¢ƒä¸­æœ‰æ­£ç¡®çš„ TF ç‰ˆæœ¬
import tensorflow as tf
from tensorflow import keras

# --- Sklearn å¯¼å…¥ ---
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# --- å…¨å±€å˜é‡å­˜å‚¨æ¨¡å‹çŠ¶æ€ ---
# è¿™äº›å°†åœ¨ initialize_engine ä¸­è¢«å¡«å……
movies_new = None
cv = None
encoder = None
feature = None
similarity = None

import os
import re
import pickle
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
import jieba
import tensorflow as tf
from tensorflow import keras
from sklearn.metrics.pairwise import cosine_similarity


# --- è¾…åŠ©å‡½æ•°ï¼šåˆ›å»º CountVectorizer ---
def _get_stopwords():
    """è¿”å›ä¸­æ–‡åœç”¨è¯åˆ—è¡¨"""
    return [
        "çš„", "äº†", "åœ¨", "æ˜¯", "æˆ‘", "æœ‰", "å’Œ", "å°±", "ä¸", "äºº",
        "éƒ½", "ä¸€", "ä¸€ä¸ª", "ä¸Š", "ä¹Ÿ", "å¾ˆ", "åˆ°", "è¯´", "è¦", "å»",
        "ä½ ", "ä¼š", "ç€", "æ²¡æœ‰", "çœ‹", "å¥½", "è‡ªå·±", "è¿™", "é‚£",
        "ä¸º", "ä¹‹", "å¯¹", "ä¸", "è€Œ", "å¹¶", "ç­‰", "è¢«", "åŠ", "æˆ–",
        "ä½†", "æ‰€ä»¥", "å¦‚æœ", "å› ä¸º", "ç„¶å", "è€Œä¸”", "é‚£ä¹ˆ", "ä»–ä»¬",
        "æˆ‘ä»¬", "ä½ ä»¬", "å®ƒä»¬", "ä»€ä¹ˆ", "å“ªä¸ª", "å“ªäº›", "å“ªé‡Œ", "æ—¶å€™",
        "ä»–", "å¥¹", "å®ƒ", "å’±ä»¬", "å¤§å®¶", "è°", "æ€æ ·", "æ€ä¹ˆ", "å¤šå°‘", "ä¸ºä»€ä¹ˆ",
        "è¿™é‡Œ", "é‚£é‡Œ", "è¿™æ ·", "é‚£æ ·", "è¿™ä¸ª", "é‚£ä¸ª", "è¿™äº›", "é‚£äº›",
        "åœ°", "å¾—", "æ‰€", "è¿‡", "å—", "å‘¢", "å§", "å•Š", "å‘€", "å˜›", "å“‡", "å•¦",
        "ä»", "è‡ª", "ä»¥", "å‘", "å…³äº", "å¯¹äº", "æ ¹æ®", "æŒ‰ç…§", "é€šè¿‡", "ç”±äº",
        "å¹¶ä¸”", "æˆ–è€…", "è™½ç„¶", "å³ä½¿", "å°½ç®¡", "ä¸ç®¡", "åªè¦", "åªæœ‰", "é™¤é",
        "æœ€", "å¤ª", "æ›´", "éå¸¸", "ååˆ†", "ç‰¹åˆ«", "æå…¶", "æ¯”è¾ƒ", "ç¨å¾®", "æœ‰ç‚¹",
        "åˆš", "æ‰", "æ­£åœ¨", "å·²ç»", "æ›¾ç»", "é©¬ä¸Š", "ç«‹åˆ»", "æ°¸è¿œ", "ä¸€ç›´", "æ€»æ˜¯",
        "å¸¸å¸¸", "ç»å¸¸", "å¾€å¾€", "ä¸æ–­", "å¶å°”", "åˆ", "å†", "è¿˜", "ä»…", "å…‰",
        "èƒ½", "èƒ½å¤Ÿ", "å¯ä»¥", "å¯èƒ½", "åº”è¯¥", "åº”å½“", "æƒ³", "æ„¿æ„", "è‚¯", "æ•¢",
        "æ¥", "å»", "è¿›", "å‡º", "å›", "èµ·", "å¼€",
        "äº›", "ä¸€äº›", "æ‰€æœ‰", "æ¯ä¸ª", "æŸä¸ª", "å„ç§", "å¤šä¸ª", "å‡ ä¸ª", "ç¬¬ä¸€", "ç¬¬äºŒ",
        "å°±æ˜¯", "åªæ˜¯", "å¯æ˜¯", "çœŸæ˜¯", "ä¹Ÿæ˜¯", "ä¸æ˜¯", "æ­£æ˜¯",
        "ä¸€æ ·", "ä¸€èˆ¬", "ä¸€ç‚¹", "ä¸€èµ·", "ä¸€ç›´", "ä¸€ä¸‹", "ä¸€ç§", "ä¸€æ¬¡"
    ]


def _create_count_vectorizer():
    """åˆ›å»ºå¹¶è¿”å›é…ç½®å¥½çš„ CountVectorizer å®ä¾‹"""
    stopwords = _get_stopwords()
    cv = CountVectorizer(
        max_features=10000,
        tokenizer=lambda text: jieba.lcut(str(text)),
        stop_words=stopwords,
        token_pattern=None
    )
    return cv


# --- ä¸»åˆå§‹åŒ–å‡½æ•° ---
# å‡è®¾ movies_new, encoder, feature, similarity, _build_encoder_structure æ˜¯åœ¨æ¨¡å—çº§åˆ«å®šä¹‰çš„å…¨å±€å˜é‡æˆ–å‡½æ•°
# from somewhere import movies_new, encoder, feature, similarity, _build_encoder_structure

def initialize_engine(data_folder_path, model_cache_path="model_cache.pkl"):
    """
    åˆå§‹åŒ–æ¨èå¼•æ“ï¼šåŠ è½½æ•°æ®ã€é¢„å¤„ç†ã€è®­ç»ƒDVAEæ¨¡å‹ï¼ˆå¦‚æœç¼“å­˜ä¸å­˜åœ¨ï¼‰ã€‚
    """
    # å£°æ˜éœ€è¦ä¿®æ”¹çš„å…¨å±€å˜é‡
    global movies_new, cv, encoder, feature, similarity
    # æ³¨æ„ï¼š'encoder' åªéœ€å£°æ˜ä¸€æ¬¡ï¼Œå¦‚æœä¹‹å‰å·²å£°æ˜è¿‡ï¼Œè¯·åˆ é™¤é‡å¤çš„ global encoder

    cache_exists = os.path.exists(model_cache_path)
    if cache_exists:
        print("ğŸ” å°è¯•ä»ç¼“å­˜åŠ è½½é¢„å¤„ç†æ¨¡å‹å’Œç‰¹å¾...")
        try:
            with open(model_cache_path, 'rb') as f:
                cache = pickle.load(f)
                movies_new = cache['movies_new']
                # cv ä¸å†ä»ç¼“å­˜åŠ è½½ï¼Œå› ä¸ºå®ƒåŒ…å«ä¸å¯ pickle çš„ lambda
                # cv = cache['cv'] # <-- åˆ é™¤æ­¤è¡Œ

                feature = cache['feature']
                similarity = cache['similarity']
                
                # encoder ä¸åºåˆ—åŒ–ï¼Œéœ€è¦é‡å»ºç»“æ„å†åŠ è½½æƒé‡
                # è¿™é‡Œå‡è®¾ _build_encoder_structure å·²æ­£ç¡®å®šä¹‰
                _build_encoder_structure(cache['inp_dim'], cache['code_dim']) 
                encoder.load_weights(os.path.join(os.path.dirname(model_cache_path), 'encoder_weights.h5'))
                
                # æ— è®ºæ˜¯å¦ä»ç¼“å­˜åŠ è½½ï¼Œéƒ½éœ€è¦é‡æ–°åˆ›å»º cv
                # å› ä¸ºå®ƒä¸è¢«ç¼“å­˜ä¸”åŒ…å« lambda
                cv = _create_count_vectorizer() # <-- é‡æ–°åˆ›å»º CountVectorizer
                
                print("âœ… æˆåŠŸä»ç¼“å­˜åŠ è½½!")
                return # <-- æˆåŠŸåŠ è½½åç›´æ¥è¿”å›ï¼Œæ— éœ€æ‰§è¡Œä¸‹é¢çš„åˆå§‹åŒ–æµç¨‹
        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜åŠ è½½å¤±è´¥: {e}ï¼Œå°†é‡æ–°è®¡ç®—...")

    print("ğŸ”„ å¼€å§‹é¢„å¤„ç†æ•°æ®å’Œè®­ç»ƒæ¨¡å‹...")

    # 1. è¯»å…¥åŸå§‹æ•°æ®
    movies_path = os.path.join(data_folder_path, "movies.csv")
    movies_db_path = os.path.join(data_folder_path, "movies_db.csv")
    director_label_path = os.path.join(data_folder_path, "director_label.csv")

    movies = pd.read_csv(movies_path)
    movies_db = pd.read_csv(movies_db_path)

    # 2. æ¸…æ´— movies_dbï¼Œæ„é€  INFO
    movies_db = movies_db.drop(columns=["durations", "votes"])
    movies_db["INFO"] = (
        movies_db["genres"].fillna("").astype(str) + " " +
        movies_db["countries"].fillna("").astype(str) + " " +
        movies_db["reviews"].fillna("").astype(str)
    )
    movies_db = movies_db.drop(columns=["genres", "countries", "reviews"])
    movies_db["title"] = movies_db["title"].apply(
        lambda x: "".join(re.findall(r"[\u4e00-\u9fff]+", str(x)))
    )

    # 3. æ¸…æ´— moviesï¼Œæœ¬ä½“åªä¿ç•™é«˜åˆ†ç”µå½±
    movies = movies.drop(
        columns=[
            "COVER", "IMDB_ID", "MINS", "OFFICIAL_SITE", "RELEASE_DATE",
            "SLUG", "ACTOR_IDS", "DIRECTOR_IDS", "LANGUAGES", "GENRES",
            "ALIAS", "ACTORS"
        ]
    )
    movies = movies[movies["DOUBAN_SCORE"] >= 6.5]

    # 4. æ„é€  movies_newï¼ˆè¯„åˆ†/äººæ•°è¿‡æ»¤ï¼‰
    movies_new_filtered = movies[movies["DOUBAN_VOTES"] >= 3000] \
        .sort_values(by=["DOUBAN_SCORE", "DOUBAN_VOTES"], ascending=[False, False])[
        ["DIRECTORS", "MOVIE_ID", "NAME", "DOUBAN_SCORE",
         "STORYLINE", "TAGS", "REGIONS", "YEAR"]
    ]

    # 5. æ‹¼æ¥å‰§æƒ… + æ ‡ç­¾ + åœ°åŒº ä½œä¸º INFO
    movies_new_filtered["INFO"] = (
        movies_new_filtered["STORYLINE"].fillna("").astype(str) + " " +
        movies_new_filtered["TAGS"].fillna("").astype(str) + " " +
        movies_new_filtered["REGIONS"].fillna("").astype(str)
    )
    movies_new_filtered = movies_new_filtered.drop(columns=["STORYLINE", "TAGS", "REGIONS"])

    # 6. æ‹¼æ¥ movies_dbï¼ˆçˆ¬è™«æ¥çš„æ•°æ®ï¼‰
    movies_db_renamed = movies_db.rename(columns={
        "subject_id": "MOVIE_ID",
        "title": "NAME",
        "year": "YEAR",
        "rating": "DOUBAN_SCORE",
        "directors": "DIRECTORS",
    })
    movies_db_renamed = movies_db_renamed[
        ["DIRECTORS", "MOVIE_ID", "NAME", "DOUBAN_SCORE", "YEAR", "INFO"]
    ]

    movies_new_combined = pd.concat([movies_new_filtered, movies_db_renamed], ignore_index=True)

    # 7. åŠ å¯¼æ¼”æ ‡ç­¾
    director_label = pd.read_csv(director_label_path)
    director_to_label = dict(zip(director_label["DIRECTOR"], director_label["LABEL"]))
    movies_new_combined["LABEL"] = movies_new_combined["DIRECTORS"].apply(
        lambda x: ",".join(
            {
                director_to_label.get(d.strip())
                for d in str(x).split("/")
                if director_to_label.get(d.strip())
            }
        ) if pd.notna(x) else None
    )

    # æ›´æ–°å…¨å±€å˜é‡ movies_new
    movies_new = movies_new_combined
    print("âœ… æ•°æ®æ¸…æ´—å®Œæˆ")

    # --- BOW + DVAE ---
    # ä½¿ç”¨è¾…åŠ©å‡½æ•°åˆ›å»º CountVectorizer
    cv = _create_count_vectorizer()

    vector = cv.fit_transform(movies_new["INFO"].astype(str)).toarray().astype("float32")
    print("âœ… BOW å‘é‡æ„å»ºå®Œæˆ")

    # DVAE å‚æ•°
    inp_dim = vector.shape[1]
    code_dim = 64
    epochs = 5  # è°ƒè¯•é˜¶æ®µè®¾å°ï¼Œç”Ÿäº§å¯è°ƒå¤§
    batch_size = 256
    beta_kl = 1.0

    # ç¼–ç å™¨
    inputs = keras.Input(shape=(inp_dim,), name="bow_counts")
    x = keras.layers.GaussianNoise(0.15)(inputs)
    x = keras.layers.Dense(1000, activation="selu")(x)
    x = keras.layers.Dense(256, activation="selu")(x)
    z_mean = keras.layers.Dense(code_dim, name="z_mean")(x)
    z_logvar = keras.layers.Dense(code_dim, name="z_logvar")(x)

    def reparameterize(args):
        mu, logvar = args
        eps = tf.random.normal(shape=tf.shape(mu))
        return mu + tf.exp(0.5 * logvar) * eps

    z = keras.layers.Lambda(reparameterize, name="z")([z_mean, z_logvar])
    encoder = keras.Model(inputs, [z_mean, z_logvar, z], name="dvae_encoder")

    # è§£ç å™¨ (ç”¨äºè®­ç»ƒ)
    latent_inputs = keras.Input(shape=(code_dim,), name="z_in")
    d = keras.layers.Dense(256, activation="selu")(latent_inputs)
    d = keras.layers.Dense(1000, activation="selu")(d)
    recons = keras.layers.Dense(inp_dim, activation=None, name="recon")(d)
    decoder = keras.Model(latent_inputs, recons, name="dvae_decoder")

    # KL æ­£åˆ™å±‚
    class KLDivergenceLayer(keras.layers.Layer):
        def __init__(self, beta=1.0, scale=1.0, **kwargs):
            super().__init__(**kwargs)
            self.beta = beta
            self.scale = scale

        def call(self, inputs):
            mu, logvar = inputs
            kl_per_sample = -0.5 * tf.reduce_sum(
                1.0 + logvar - tf.exp(logvar) - tf.square(mu), axis=1
            )
            kl = tf.reduce_mean(kl_per_sample) / float(self.scale)
            self.add_loss(self.beta * kl)
            return tf.zeros_like(mu[:, :1])

    z_mean_out, z_logvar_out, z_out = encoder(inputs)
    _ = KLDivergenceLayer(beta=beta_kl, scale=inp_dim, name="kl_reg")(
        [z_mean_out, z_logvar_out]
    )
    recons_out = decoder(z_out)

    vae = keras.Model(inputs, recons_out, name="dvae")
    vae.compile(optimizer=keras.optimizers.Adam(1e-3), loss="mse")

    # è®­ç»ƒ VAE
    history = vae.fit(
        vector, vector,
        epochs=epochs,
        batch_size=batch_size,
        validation_split=0.1,
        verbose=1,
    )
    print("âœ… DVAE æ¨¡å‹è®­ç»ƒå®Œæˆ")

    # æå–ç”µå½±è¯­ä¹‰å‘é‡ featureï¼ˆz_meanï¼‰
    z_mean_val = encoder.predict(vector, verbose=0)[0]
    feature = z_mean_val.astype("float32")
    print("âœ… ç”µå½±è¯­ä¹‰ç‰¹å¾æå–å®Œæˆ")

    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ
    similarity = cosine_similarity(feature)
    print("âœ… ç›¸ä¼¼åº¦çŸ©é˜µè®¡ç®—å®Œæˆ")

    # --- ç¼“å­˜æ¨¡å‹å’Œç‰¹å¾ ---
    print("ğŸ’¾ æ­£åœ¨ç¼“å­˜æ¨¡å‹å’Œç‰¹å¾...")
    # æ³¨æ„ï¼šä¸å†ç¼“å­˜ 'cv' å¯¹è±¡ï¼Œå› ä¸ºå®ƒåŒ…å«äº†ä¸å¯ pickle çš„ lambda
    cache_to_save = {
        'movies_new': movies_new,     # DataFrame
        # 'cv': cv,                   # <-- ç§»é™¤æ­¤è¡Œ
        'feature': feature,           # NumPy array
        'similarity': similarity,     # NumPy array
        'inp_dim': inp_dim,           # int (ç”¨äºé‡å»º encoder ç»“æ„)
        'code_dim': code_dim          # int (ç”¨äºé‡å»º encoder ç»“æ„)
        # å¦‚æœéœ€è¦ç¼“å­˜ director_to_labelï¼Œä¹Ÿå¯ä»¥åŠ ä¸Š
        # 'director_to_label': director_to_label 
    }
    
    try:
        with open(model_cache_path, 'wb') as f:
            pickle.dump(cache_to_save, f)
        encoder.save_weights(os.path.join(os.path.dirname(model_cache_path), 'encoder.weights.h5'))
        print("âœ… ç¼“å­˜ä¿å­˜æˆåŠŸ!")
    except Exception as e:
        print(f"âš ï¸ ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
        # æ ¹æ®ä½ çš„éœ€æ±‚å†³å®šæ˜¯å¦è¦åœ¨è¿™é‡ŒæŠ›å‡ºå¼‚å¸¸
        # raise e # å¦‚æœç¼“å­˜å¤±è´¥æ˜¯è‡´å‘½é”™è¯¯ï¼Œå–æ¶ˆæ³¨é‡Šæ­¤è¡Œ

    # æ³¨æ„ï¼šå‡½æ•°ç»“æŸï¼Œcv å·²åœ¨æ­¤å‡½æ•°ä½œç”¨åŸŸå†…åˆ›å»ºå¹¶èµ‹å€¼ç»™å…¨å±€å˜é‡

def _build_encoder_structure(inp_dim, code_dim):
    """é‡å»ºç¼–ç å™¨ç»“æ„ä»¥ä¾¿åŠ è½½æƒé‡"""
    global encoder
    inputs = keras.Input(shape=(inp_dim,), name="bow_counts")
    x = keras.layers.GaussianNoise(0.15)(inputs)
    x = keras.layers.Dense(1000, activation="selu")(x)
    x = keras.layers.Dense(256, activation="selu")(x)
    z_mean = keras.layers.Dense(code_dim, name="z_mean")(x)
    z_logvar = keras.layers.Dense(code_dim, name="z_logvar")(x)

    def reparameterize(args):
        mu, logvar = args
        eps = tf.random.normal(shape=tf.shape(mu))
        return mu + tf.exp(0.5 * logvar) * eps

    z = keras.layers.Lambda(reparameterize, name="z")([z_mean, z_logvar])
    encoder = keras.Model(inputs, [z_mean, z_logvar, z], name="dvae_encoder")


def get_movie_features():
    """è·å–ç”µå½±ç‰¹å¾å‘é‡"""
    return feature


def get_movies_dataframe():
    """è·å–å¤„ç†åçš„ç”µå½±DataFrame"""
    return movies_new


def get_similarity_matrix():
    """è·å–ç”µå½±ç›¸ä¼¼åº¦çŸ©é˜µ"""
    return similarity


def recommand(movie_name, sample_top=15, pick_n=5):
    """åŸºç¡€æ¨èå‡½æ•°ï¼ˆåªç”¨å†…å®¹ç›¸ä¼¼ï¼‰"""
    label_idx = movies_new.index[movies_new["NAME"] == movie_name]
    if len(label_idx) == 0:
        # å°è¯•æ¨¡ç³ŠåŒ¹é…
        similar_movies = movies_new[movies_new["NAME"].str.contains(movie_name, na=False, case=False)]
        if len(similar_movies) > 0:
            print(f"æœªç²¾ç¡®æ‰¾åˆ°ã€Š{movie_name}ã€‹ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…:")
            for idx, row in similar_movies.head(3).iterrows():
                 print(f"  - {row['NAME']}")
            # é»˜è®¤ä½¿ç”¨ç¬¬ä¸€ä¸ªåŒ¹é…é¡¹
            pos = similar_movies.index[0]
        else:
            print(f"æœªæ‰¾åˆ°å½±ç‰‡ï¼šã€Š{movie_name}ã€‹")
            return None
    else:
        pos = movies_new.index.get_loc(label_idx[0])

    sims = similarity[pos]
    cand = np.argsort(-sims)  # é™åº
    cand = cand[cand != pos]  # å»æ‰è‡ªèº«
    top_candidates = cand[:sample_top]

    n_pick = min(pick_n, len(top_candidates))
    if n_pick == 0:
        return pd.DataFrame()
    selected = np.random.choice(top_candidates, n_pick, replace=False)

    recs = []
    for j in selected:
        recs.append({
            "ç”µå½±å": movies_new.iloc[j]["NAME"],
            "è±†ç“£è¯„åˆ†": movies_new.iloc[j]["DOUBAN_SCORE"],
            "æµæ´¾": movies_new.iloc[j]["LABEL"],
            "ç›¸ä¼¼åº¦": sims[j],
            "å¯¼æ¼”": movies_new.iloc[j]["DIRECTORS"],
        })
    df = pd.DataFrame(recs).sort_values(by="ç›¸ä¼¼åº¦", ascending=False).reset_index(drop=True)
    return df