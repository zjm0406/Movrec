{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e90681ce-274f-45ea-b363-2cc59f966c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "movies=pd.read_csv('movies.csv')\n",
    "movies_db=pd.read_csv('movies_db.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7d32fe-8598-4d77-a276-0c161b824e04",
   "metadata": {},
   "source": [
    "### 1.数据清洗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a9e8a82f-7927-4898-bada-648184ed3d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1172 entries, 0 to 1171\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   subject_id  1172 non-null   object\n",
      " 1   title       1172 non-null   object\n",
      " 2   year        1172 non-null   object\n",
      " 3   rating      1168 non-null   object\n",
      " 4   directors   1169 non-null   object\n",
      " 5   INFO        1172 non-null   object\n",
      "dtypes: object(6)\n",
      "memory usage: 55.1+ KB\n"
     ]
    }
   ],
   "source": [
    "movies_db=movies_db.drop(\n",
    "    columns=['durations','votes']\n",
    ")\n",
    "movies_db['INFO'] = (\n",
    "    movies_db['genres'].fillna('').astype(str) + ' ' +\n",
    "    movies_db['countries'].fillna('').astype(str) + ' ' +\n",
    "    movies_db['reviews'].fillna('').astype(str)\n",
    ")\n",
    "\n",
    "movies_db = movies_db.drop(columns=['genres', 'countries', 'reviews'])\n",
    "movies_db['title'] = movies_db['title'].apply(lambda x: ''.join(re.findall(r'[\\u4e00-\\u9fff]+', str(x))))\n",
    "movies_db.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e5479d-a17e-4ca0-8922-d2a80c3439ba",
   "metadata": {},
   "source": [
    "删除无用列并只保留评分大于6.5的电影"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c5e5bc38-02d2-4c94-a462-55a568c45ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = movies.drop(\n",
    "    columns=['COVER','IMDB_ID','MINS','OFFICIAL_SITE','RELEASE_DATE','SLUG','ACTOR_IDS','DIRECTOR_IDS','LANGUAGES','GENRES','ALIAS','ACTORS']\n",
    ")\n",
    "movies = movies[movies['DOUBAN_SCORE'] >= 6.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12251fd-378e-479a-9c3c-357c235ec45b",
   "metadata": {},
   "source": [
    "筛选评分人数大于5000的电影并降序排列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "100e8223-7846-4eec-930a-e8434df5b53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_new = movies[(movies['DOUBAN_VOTES'] >= 3000)].sort_values(by=['DOUBAN_SCORE','DOUBAN_VOTES'], ascending=[False,False])[['DIRECTORS','MOVIE_ID','NAME','DOUBAN_SCORE','STORYLINE','TAGS','REGIONS','YEAR']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f280483-5565-4977-836b-4cd4dce27a0a",
   "metadata": {},
   "source": [
    "### 2.余弦相似度模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eedd4aa0-ef2f-41a2-ba96-c0c59125c574",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_new['INFO'] = (\n",
    "    movies['STORYLINE'].fillna('').astype(str) + \" \" +\n",
    "    movies['TAGS'].fillna('').astype(str) + \" \" +\n",
    "    movies['REGIONS'].fillna('').astype(str)\n",
    "#    + \" \" +\n",
    "#    movies['DIRECTORS'].fillna('').astype(str) + \" \" +\n",
    "#    movies['ACTORS'].fillna('').astype(str) + \" \" +\n",
    "#    movies['YEAR'].fillna('').astype(str)\n",
    ")\n",
    "movies_new = movies_new.drop(columns=['STORYLINE', 'TAGS', 'REGIONS'\n",
    "                                      #, 'DIRECTORS', 'ACTORS', 'YEAR'\n",
    "                                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5fe8b7f6-9dc8-4105-b2ff-d58e59db50e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5058 entries, 0 to 5057\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   DIRECTORS     4913 non-null   object\n",
      " 1   MOVIE_ID      5058 non-null   object\n",
      " 2   NAME          5058 non-null   object\n",
      " 3   DOUBAN_SCORE  5054 non-null   object\n",
      " 4   YEAR          5058 non-null   object\n",
      " 5   INFO          5058 non-null   object\n",
      "dtypes: object(6)\n",
      "memory usage: 237.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "movies_db_renamed = movies_db.rename(columns={\n",
    "    'subject_id': 'MOVIE_ID',\n",
    "    'title': 'NAME',\n",
    "    'year': 'YEAR',\n",
    "    'rating': 'DOUBAN_SCORE',\n",
    "    'directors': 'DIRECTORS'\n",
    "})\n",
    "movies_db_renamed = movies_db_renamed[['DIRECTORS', 'MOVIE_ID', 'NAME', 'DOUBAN_SCORE', 'YEAR', 'INFO']]\n",
    "movies_new = pd.concat([movies_new, movies_db_renamed], ignore_index=True)\n",
    "print(movies_new.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ea069d99-9363-4741-8bd3-32b733adab9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#添加中文分词库\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "308629b3-bc33-486b-bb5a-64e9dff3ea10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 中文分词器\n",
    "def chinese_tokenizer(text):\n",
    "    return jieba.lcut(str(text))\n",
    "stopwords = [\n",
    "    \"的\", \"了\", \"在\", \"是\", \"我\", \"有\", \"和\", \"就\", \"不\", \"人\", \n",
    "    \"都\", \"一\", \"一个\", \"上\", \"也\", \"很\", \"到\", \"说\", \"要\", \"去\",\n",
    "    \"你\", \"会\", \"着\", \"没有\", \"看\", \"好\", \"自己\", \"这\", \"那\", \n",
    "    \"为\", \"之\", \"对\", \"与\", \"而\", \"并\", \"等\", \"被\", \"及\", \"或\",\n",
    "    \"但\", \"所以\", \"如果\", \"因为\", \"然后\", \"而且\", \"那么\", \"他们\", \n",
    "    \"我们\", \"你们\", \"它们\", \"什么\", \"哪个\", \"哪些\", \"哪里\", \"时候\",\n",
    "    \"他\", \"她\", \"它\", \"咱们\", \"大家\", \"谁\", \"怎样\", \"怎么\", \"多少\", \"为什么\",\n",
    "    \"这里\", \"那里\", \"这样\", \"那样\", \"这个\", \"那个\", \"这些\", \"那些\",\n",
    "    \"地\", \"得\", \"所\", \"过\", \"吗\", \"呢\", \"吧\", \"啊\", \"呀\", \"嘛\", \"哇\", \"啦\",\n",
    "    \"从\", \"自\", \"以\", \"向\", \"关于\", \"对于\", \"根据\", \"按照\", \"通过\", \"由于\",\n",
    "    \"并且\", \"或者\", \"虽然\", \"即使\", \"尽管\", \"不管\", \"只要\", \"只有\", \"除非\",\n",
    "    \"最\", \"太\", \"更\", \"非常\", \"十分\", \"特别\", \"极其\", \"比较\", \"稍微\", \"有点\",\n",
    "    \"刚\", \"才\", \"正在\", \"已经\", \"曾经\", \"马上\", \"立刻\", \"永远\", \"一直\", \"总是\",\n",
    "    \"常常\", \"经常\", \"往往\", \"不断\", \"偶尔\", \"又\", \"再\", \"还\", \"仅\", \"光\",\n",
    "    \"能\", \"能够\", \"可以\", \"可能\", \"应该\", \"应当\", \"想\", \"愿意\", \"肯\", \"敢\",\n",
    "    \"来\", \"去\", \"进\", \"出\", \"回\", \"起\", \"开\",\n",
    "    \"些\", \"一些\", \"所有\", \"每个\", \"某个\", \"各种\", \"多个\", \"几个\", \"第一\", \"第二\",\n",
    "    \"就是\", \"只是\", \"可是\", \"真是\", \"也是\", \"不是\", \"也是\", \"就是\", \"正是\",\n",
    "    \"一样\", \"一般\", \"一点\", \"一起\", \"一直\", \"一下\", \"一些\", \"一种\", \"一次\"\n",
    "]\n",
    "cv = CountVectorizer(\n",
    "    max_features=10000,\n",
    "    tokenizer=chinese_tokenizer,\n",
    "    stop_words=stopwords,   # 中文停用词表（如果有就传 list）\n",
    "    token_pattern=None      # 必须设 None，否则 tokenizer 会被覆盖\n",
    ")\n",
    "\n",
    "vector = cv.fit_transform(movies_new['INFO'].astype(str)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "57a6fe53-971b-4ded-b2ce-f0b2663f3b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 248ms/step - loss: 0.0486 - val_loss: 0.0909\n",
      "Epoch 2/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 234ms/step - loss: 0.0204 - val_loss: 0.0637\n",
      "Epoch 3/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 251ms/step - loss: 0.0152 - val_loss: 0.0466\n",
      "Epoch 4/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 241ms/step - loss: 0.0131 - val_loss: 0.0405\n",
      "Epoch 5/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 248ms/step - loss: 0.0123 - val_loss: 0.0629\n",
      "Epoch 6/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 231ms/step - loss: 0.0119 - val_loss: 0.0377\n",
      "Epoch 7/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 267ms/step - loss: 0.0113 - val_loss: 0.0337\n",
      "Epoch 8/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 262ms/step - loss: 0.0112 - val_loss: 0.0325\n",
      "Epoch 9/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 246ms/step - loss: 0.0107 - val_loss: 0.0311\n",
      "Epoch 10/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 251ms/step - loss: 0.0103 - val_loss: 0.0310\n",
      "Epoch 11/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 235ms/step - loss: 0.0102 - val_loss: 0.0290\n",
      "Epoch 12/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 225ms/step - loss: 0.0100 - val_loss: 0.0291\n",
      "Epoch 13/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 223ms/step - loss: 0.0099 - val_loss: 0.0288\n",
      "Epoch 14/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 253ms/step - loss: 0.0097 - val_loss: 0.0286\n",
      "Epoch 15/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 226ms/step - loss: 0.0096 - val_loss: 0.0278\n",
      "Epoch 16/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 223ms/step - loss: 0.0094 - val_loss: 0.0279\n",
      "Epoch 17/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 213ms/step - loss: 0.0095 - val_loss: 0.0273\n",
      "Epoch 18/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 231ms/step - loss: 0.0094 - val_loss: 0.0273\n",
      "Epoch 19/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 227ms/step - loss: 0.0092 - val_loss: 0.0269\n",
      "Epoch 20/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 226ms/step - loss: 0.0092 - val_loss: 0.0266\n"
     ]
    }
   ],
   "source": [
    "# ========= 去噪变分自编码器（DVAE）— 稳定版（自定义Layer加KL） =========\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "inp_dim    = vector.shape[1]\n",
    "code_dim   = 64            # 可改 64/128\n",
    "epochs     = 20\n",
    "batch_size = 256\n",
    "beta_kl    = 1.0          # β-VAE 系数\n",
    "\n",
    "# 确保 dtype 稳定\n",
    "vector = vector.astype(\"float32\", copy=False)\n",
    "\n",
    "# --- 编码器：输入端去噪 ---\n",
    "inputs = keras.Input(shape=(inp_dim,), name=\"bow_counts\")\n",
    "x = keras.layers.GaussianNoise(0.15)(inputs)          # 去噪；也可换 Dropout(0.3)\n",
    "x = keras.layers.Dense(1000, activation=\"selu\")(x)\n",
    "x = keras.layers.Dense(256,  activation=\"selu\")(x)\n",
    "z_mean   = keras.layers.Dense(code_dim, name=\"z_mean\")(x)\n",
    "z_logvar = keras.layers.Dense(code_dim, name=\"z_logvar\")(x)\n",
    "\n",
    "def reparameterize(args):\n",
    "    mu, logvar = args\n",
    "    eps = tf.random.normal(shape=tf.shape(mu))\n",
    "    return mu + tf.exp(0.5 * logvar) * eps\n",
    "\n",
    "z = keras.layers.Lambda(reparameterize, name=\"z\")([z_mean, z_logvar])\n",
    "\n",
    "# 打包一个 encoder（包含三个输出：mean, logvar, z）\n",
    "encoder = keras.Model(inputs, [z_mean, z_logvar, z], name=\"dvae_encoder\")\n",
    "\n",
    "# --- 解码器：线性输出，用 MSE 重构计数 ---\n",
    "latent_inputs = keras.Input(shape=(code_dim,), name=\"z_in\")\n",
    "d = keras.layers.Dense(256,  activation=\"selu\")(latent_inputs)\n",
    "d = keras.layers.Dense(1000, activation=\"selu\")(d)\n",
    "recons = keras.layers.Dense(inp_dim, activation=None, name=\"recon\")(d)\n",
    "decoder = keras.Model(latent_inputs, recons, name=\"dvae_decoder\")\n",
    "\n",
    "# --- 自定义 KL 层：把 KL 正则通过 layer.add_loss() 注入 ---\n",
    "class KLDivergenceLayer(keras.layers.Layer):\n",
    "    def __init__(self, beta=1.0, scale=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.beta = beta\n",
    "        self.scale = scale  # 用于与 MSE 标度对齐\n",
    "\n",
    "    def call(self, inputs):\n",
    "        mu, logvar = inputs\n",
    "        # KL = -0.5 * sum(1 + logvar - exp(logvar) - mu^2)\n",
    "        kl_per_sample = -0.5 * tf.reduce_sum(\n",
    "            1.0 + logvar - tf.exp(logvar) - tf.square(mu), axis=1\n",
    "        )\n",
    "        kl = tf.reduce_mean(kl_per_sample) / float(self.scale)\n",
    "        self.add_loss(self.beta * kl)\n",
    "        # 返回个“占位输出”，不参与后续计算\n",
    "        return tf.zeros_like(mu[:, :1])\n",
    "\n",
    "# --- 组装 DVAE：重构 + KL层（只为加loss，不改计算图）---\n",
    "z_mean_out, z_logvar_out, z_out = encoder(inputs)\n",
    "_ = KLDivergenceLayer(beta=beta_kl, scale=inp_dim, name=\"kl_reg\")([z_mean_out, z_logvar_out])\n",
    "recons_out = decoder(z_out)\n",
    "\n",
    "vae = keras.Model(inputs, recons_out, name=\"dvae\")\n",
    "\n",
    "# --- 训练：重构用 MSE；KL 已由 KL 层注入 ---\n",
    "vae.compile(optimizer=keras.optimizers.Adam(1e-3), loss=\"mse\")\n",
    "vae.fit(vector, vector, epochs=epochs, batch_size=batch_size, validation_split=0.1, verbose=1)\n",
    "\n",
    "# --- 用 z_mean 作为电影向量（更稳、更可插值）---\n",
    "z_mean_val = encoder.predict(vector, verbose=0)[0]   # 形状: [N, code_dim]\n",
    "feature = z_mean_val\n",
    "\n",
    "# ========= DVAE 结束 =========\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity = cosine_similarity(feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3d96bd8f-46b9-4c89-8143-6cd5ac72dae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "52e7374e-5af3-4825-8f95-e94e501adba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommand(movie_name, sample_top=15, pick_n=5):\n",
    "    label_idx = movies_new.index[movies_new['NAME'] == movie_name]\n",
    "    if len(label_idx) == 0:\n",
    "        print(\"未找到该影片\")\n",
    "        return\n",
    "    pos = movies_new.index.get_loc(label_idx[0])\n",
    "\n",
    "    sims = similarity[pos]\n",
    "    cand = np.argsort(-sims)   # 按相似度降序排列索引\n",
    "    cand = cand[cand != pos]   # 去掉自身\n",
    "    top_candidates = cand[:sample_top]  # 取前15个\n",
    "    \n",
    "    # 如果数量足够，从前15中随机选5个（无放回）\n",
    "    n_pick = min(pick_n, len(top_candidates))\n",
    "    selected = np.random.choice(top_candidates, n_pick, replace=False)\n",
    "\n",
    "    recs = []\n",
    "    for j in selected:\n",
    "        recs.append({\n",
    "            \"电影名\": movies_new.iloc[j]['NAME'],\n",
    "            \"豆瓣评分\": movies_new.iloc[j]['DOUBAN_SCORE'],\n",
    "            \"相似度\": sims[j]\n",
    "        })\n",
    "    df = pd.DataFrame(recs).sort_values(by=\"相似度\", ascending=False).reset_index(drop=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2d1a0c22-b1c2-4f17-b6ab-8d8ee94ed40b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>电影名</th>\n",
       "      <th>豆瓣评分</th>\n",
       "      <th>相似度</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>钢铁侠</td>\n",
       "      <td>8.2</td>\n",
       "      <td>0.943882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>情迷六月花</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.942665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>美国派2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.939954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>秘密与谎言</td>\n",
       "      <td>8.2</td>\n",
       "      <td>0.938467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>心慌方</td>\n",
       "      <td>7.9</td>\n",
       "      <td>0.937128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     电影名  豆瓣评分       相似度\n",
       "0    钢铁侠   8.2  0.943882\n",
       "1  情迷六月花   7.5  0.942665\n",
       "2   美国派2   7.0  0.939954\n",
       "3  秘密与谎言   8.2  0.938467\n",
       "4    心慌方   7.9  0.937128"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommand(\"苦月亮\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
